{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9445466b-46ea-494a-aba8-9954aee43568",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-07 10:53:09.955388: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import scipy.sparse\n",
    "import scipy.linalg\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554018aa-3660-4150-854b-e74b103d7828",
   "metadata": {},
   "source": [
    "# Inverse Problem\n",
    "Given parameters of interest (PoI) and observation of a state variable, consider the folowing additive noise model\n",
    "$$\n",
    "\\textbf{y} = F(\\textbf{u}) + \\textbf{e}\n",
    "$$\n",
    "where $\\textbf{u}$ are the PoI, $F$ is the parameter-to-observable(Pto) map and $\\textbf{y}$ represent observational data.  \n",
    "\n",
    "__Goal:__ determine the parameter of interest $\\textbf{u}$ given the observational data $\\textbf{y}$.  \n",
    "\n",
    "Solving the inverse problem usually involves solving an optimization problem of the form\n",
    "$$\n",
    "\\min_{\\textbf{u}} \\lVert \\textbf{y} - F(\\textbf{u}) \\rVert_{2}^2 + R(\\textbf{u})\n",
    "$$\n",
    "where $ R(\\textbf{u})$ is a regularization term to reduce the size of the solution space, since the problem is usually ill-posed (many possible solution exist that are coherent with our data).  \n",
    "\n",
    "__Problem:__ the optimization of such functional is computationally expansive.  \n",
    "\n",
    "__Solution:__ learning a data-driven solver that after an offline training stage (expansive but done only one time), output estimate of our PoI.  \n",
    "\n",
    "Given as training set  $\\{(\\textbf{u}^{(m)}, \\textbf{y}^{(m)})\\}_{m=1}^M$, if we use a neural network $\\Psi$ then our solver is paramerized by the weight of the network $\\textbf{W}$. This solver require the optimization of the following functional\n",
    "$$\n",
    "\\min_{\\textbf{W}} \\frac{1}{M} \\sum_{m=1}^M \\lVert \\textbf{u}^{(m)} - \\Psi(\\textbf{y}^{(m)}, \\textbf{W}) \\rVert_{2}^2 + R(\\textbf{W})\n",
    "$$\n",
    "\n",
    "Instead of regularize the weight of the network directly, we can regularize the output of the network, informing the optimization procedure of the inversion task, of the properties of the noise afflicting our observational data and the knowledge about some physical properties of the PoI we posses. This lead to the following optimization objective\n",
    "$$\n",
    "\\min_{\\textbf{W}} \\frac{1}{M} \\sum_{m=1}^M \\lVert \\textbf{u}^{(m)} - \\Psi(\\textbf{y}^{(m)}, \\textbf{W}) \\rVert_{2}^2 + \\lVert M (\\textbf{y} - F(\\Psi(\\textbf{y}^{(m)}, \\textbf{W}))) \\rVert_{2}^2 + \\lVert P (\\Psi(\\textbf{y}^{(m)}, \\textbf{W})) \\rVert_{2}^2\n",
    "$$\n",
    "where $M,P$ are some mapping representing information about noise and PoI respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32247cb6-e1eb-4a8e-b6fc-c32b80fe753f",
   "metadata": {},
   "source": [
    "# Bayesian Inverse Problem\n",
    "By adopting a probabilisitc framework instead of deterministic, the question asked by the inverse problem essentially changes from “what is the value of our parameter?” to “how accurate is the estimate of our parameter?”.   \n",
    "\n",
    "In this setting, inverse problem deal with the following observational model\n",
    "$$\n",
    "    Y = F(U) + E\n",
    "$$\n",
    "where $F$ is __Parameter-to-Observable(PtO)__ map and $Y,U,E$ are random variable representing respectively the observational data, the __Parameters of Intereste(PoI)__ and the noise model.  \n",
    "\n",
    " \n",
    "\n",
    "__Goal:__ model the posterior distribution $P(\\textbf{u} | \\textbf{y})$, i.e \"given the observational data, what is the distribution of the parameters of interest?\"\n",
    "\n",
    "Using Bayes' Theorem\n",
    "$$\n",
    "P(\\textbf{u} | \\textbf{y}) \\propto P(\\textbf{y} | \\textbf{u})P(\\textbf{u})\n",
    "$$\n",
    "The assumptions usually made are:\n",
    "- $E \\sim N(\\mu_E, \\Gamma_E)$, $U \\sim N(\\mu_{pr}, \\Gamma_{pr})$, $E\\perp U$\n",
    "- $ P(\\textbf{y} | \\textbf{u}) = P_E (\\textbf{y} - F(\\textbf{u}))$\n",
    "\n",
    "\n",
    "__Problem:__ computing such probability is often intractable.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dc4beb-a7dc-4f81-be68-d374156561ad",
   "metadata": {},
   "source": [
    "__Solution:__ using variational inference to approximate such distributions.\n",
    "\n",
    "Denoting by  $P(\\textbf{u} | \\textbf{y})$ the target posterior density we want to approximate, __variational inference__ perform such approximation choosing a set of probability distributin $Q_{\\phi}(\\textbf{u} | \\textbf{y})$ parameterized by a parameter $\\phi$ and then finiding the distribution in such family \"closest\" to the target one.  \n",
    "\n",
    "To do that some notion of distance between $Q_{\\phi}(\\textbf{u} | \\textbf{y})$ and $P(\\textbf{u} | \\textbf{y})$ must be introduced.  \n",
    "\n",
    "Normally the Kullback-Leibler divergence (KLD) is used to obtain the evidence lower bound (ELBO), an useful lower bound on the log-likelihood of some observed data, and the goal of variational inference is to minimize the KL divergence, or, equivalently, maximizing the evidence lower bound (ELBO).\n",
    "\n",
    "Following the work in the reference [1], instead of KLD, a family of Jensen-Shannon Divergence (JSD) is used, leading to the minimization of the following quantity\n",
    "$$\n",
    "\\frac{1-\\alpha}{\\alpha} KL(P(\\textbf{u} | \\textbf{y}) | Q_{\\phi}\\textbf{u} | \\textbf{y})) - \\mathbb{E}_{\\textbf{u} \\sim Q_{\\phi}} \\left[ \\log(P(\\textbf{y} | \\textbf{u})) \\right] + KL(Q_{\\phi}(\\textbf{u} | \\textbf{y}) |P(\\textbf{u}))\n",
    "$$\n",
    "where $KL(\\cdot,\\cdot)$ denote the Kullback-Leibler divergence.  \n",
    "\n",
    "__Objective:__  incorporate such minimization problem into a deep learning framework, i.e recave a differentiable loss function in the form of the one written above in the deterministic setting to train a neural network to perform the variational inference task. \n",
    "\n",
    "To do so:\n",
    "- consider a training set  $\\{(\\textbf{u}^{(m)}, \\textbf{y}^{(m)})\\}_{m=1}^M$\n",
    "- apply the fact that the minimization of the KLD between the empirical and model distributions is equivalent to maximization of the likelihood function with respect to $\\phi$.\n",
    "- form a Monte-Carlo estimation using our PoI data\n",
    "- adopt a Gaussian model for our model posterior $Q_{\\phi}(\\textbf{u} | \\textbf{y}^{(m)})= N(\\textbf{u} | \\mu_{post}^{(m)}, \\Gamma_{post}^{(m)})$\n",
    "- adopt the Gaussian prior distribution for the noise and the PoI model as assumed above\n",
    "\n",
    "Detail are in the reference [1]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc77a29-a907-4191-82bc-f358209a85f9",
   "metadata": {},
   "source": [
    "Considering a neural network $\\Psi$ with paramters $\\textbf{W}$ that takes as input our observation data $\\textbf{y}^{(m)}$ and output the statistics of our approximated posterior model $\\phi^{(m)} = (\\mu_{post}^{(m)}, \\Gamma_{post}^{(m)})$, the following loss function is obtained:\n",
    "$$\n",
    "\\min_{\\textbf{W}} \\frac{1}{M} \\sum_{m=1}^M \\frac{1-\\alpha}{\\alpha} (\\log |\\Gamma_{post}^{(m)}| + \\lVert \\mu_{post}^{(m)}- \\textbf{u}^{(m)}) \\rVert_{\\Gamma_{post}^{(m)-1}}^2 \n",
    "$$\n",
    "\n",
    "$$\n",
    "    + \\lVert \\textbf{y}^{(m)} - F(\\textbf{u}_{draw}^{(m)}(\\textbf{W})) - \\mu_E \\rVert_{\\Gamma_{E}^{-1}}^2 \n",
    "$$\n",
    "\n",
    "$$\n",
    "     + \\text{tr}(\\Gamma_{pr}^{-1}\\Gamma_{post}^{(m)}) +   \\lVert \\mu_{post}^{(m)}- \\textbf{u}_{(pr)}) \\rVert_{\\Gamma_{pr}^{-1}}^2 + \\log \\frac{\\Gamma_{pr}}{\\Gamma_{post}^{(m)}}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "    (\\mu_{post}^{(m)}, \\Gamma_{post}^{\\frac{1}{2}(m)}) = \\Psi (\\textbf{y}^{(m)}, \\textbf{W})\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\textbf{u}_{draw}^{(m)}(\\textbf{W}) = \\mu_{post}^{(m)} + \\Gamma_{post}^{\\frac{1}{2}(m)} \\epsilon\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\epsilon \\sim N(0, \\textbf{I}_D)\n",
    "$$\n",
    "\n",
    "During the training procedure, the repeated operation of the PtO map on our dataset $\\{(\\textbf{u}^{(m)}, \\textbf{y}^{(m)})\\}_{m=1}^M$ may incur a significant computational cost. To alleviate this the PtO map is replaced with another neural network $\\Psi_d$ parameterized by weights $\\textbf{W}_d$. This is reflected in the following modification of the loss function\n",
    "\n",
    "\n",
    "$$\n",
    "     \\lVert \\textbf{y}^{(m)} - F(\\textbf{u}_{draw}^{(m)}(\\textbf{W})) - \\mu_E \\rVert_{\\Gamma_{E}^{-1}}^2  \\rightarrow \\lVert \\textbf{y}^{(m)} - \\Psi_d(\\textbf{u}_{draw}^{(m)}(\\textbf{W}), \\textbf{W}_d) - \\mu_E \\rVert_{\\Gamma_{E}^{-1}}^2\n",
    "$$\n",
    "\n",
    "As exaplained in the paragraph below, our loss function is regularized by the likelihood model containing the PtO map and the prior model containing information on our PoI space.\n",
    "![](./assets/UQ-VAE.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b74be2f-fb12-42ee-8eff-2bda5afd2559",
   "metadata": {},
   "source": [
    "# Test\n",
    "\n",
    "## Poisson Problem\n",
    "Consider the following modification of Diriclet problem for the poisson equation in two dimension, $\\textbf{x} = (x,y) \\in \\Omega = (0,1)^2$:\n",
    "$$\n",
    "\\begin{cases}\n",
    "    - q(\\textbf{x})\\Delta y(\\textbf{x}) = f(\\textbf{x}) \\;\\;\\;\\;\\;\\;\\;\\;\\;  \\textbf{x} \\in \\Omega \\\\\n",
    "    y(\\textbf{x}) = 0 \\;\\;\\;\\;\\;\\;\\;\\;\\;  \\textbf{x} \\in \\partial \\Omega\n",
    "\\end{cases}\n",
    "$$\n",
    "where $q(\\textbf{x})$ is the conductivity coefficent, our parameter of interest (with previous notation $\\textbf{u} := q(\\textbf{x})) $.  \n",
    "\n",
    "We use a computational grid of dimension $D$, so $\\textbf{u} \\in R^D$, and $\\textbf{y} \\in R^O$, with $O$ the dimension of observational data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c71b7fb-3744-4b22-a812-acc873b6c45a",
   "metadata": {},
   "source": [
    "## Probabilistic models\n",
    "As prior model for the parameter of interest we set\n",
    "$$\n",
    "P(\\textbf{u}) = N(\\mu_{pr}, \\Gamma_{pr})\n",
    "$$\n",
    "with $\\mu_{pr} = 2 \\textbf{I}_D$ and $\\Gamma_{pr} = A^{-2}$ where $A$ is a differential operator such that\n",
    "\n",
    "$$\n",
    "Au =  \n",
    "\\begin{cases}\n",
    "     -\\gamma \\Delta u + \\delta u \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\text{in} \\;\\; \\Omega \\\\\n",
    "     \\nabla u \\cdot \\textbf{n} + \\beta u \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\text{in}\\;\\; \\partial \\Omega\n",
    "\\end{cases}   \n",
    "$$\n",
    "with $\\gamma = 0.1, \\delta = 0.5, \\beta = ...$\n",
    "\n",
    "For the noise model, we set\n",
    "$$\n",
    "P(E) = N(\\mu_{E}, \\Gamma_{E})\n",
    "$$\n",
    "with $\\mu_{E} = 0 \\textbf{I}_O$ and $\\Gamma_{E} = \\sigma^{2}\\textbf{I}_O$ with $\\sigma = \\eta \\max |\\textbf{y}|$.\n",
    "\n",
    "Finally assume a diagonal matrix for the posterior covariance $\\Gamma_{post}$. In this way the encoder network output the diagonal of such matrix as a \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3753e32b-93d9-4c2e-93c4-1efa0c689e57",
   "metadata": {},
   "source": [
    "## Dataset generation\n",
    "First, we drawn the parameter of interest $\\textbf{u} := \\textbf{q(\\textbf{x})}$ from a Gaussian process (da chiarire se posso usarlo) with mean $\\mu = 2$ and covariance\n",
    "\n",
    "$$\n",
    "\\Gamma_{i,j} = \\sigma^2 \\exp(- \\frac{\\lVert \\textbf{x}_i - \\textbf{x}_j \\rVert_2^2 }{2  \\rho^2} )\n",
    "$$\n",
    "with $\\sigma^2$, $\\rho = 0.5$.  \n",
    "\n",
    "Then we generate the corrispondonding observation $\\textbf{y}$ using the Poisson solver previusly developed.\n",
    "We generate in this way both the training set $\\{(\\textbf{u}^{(m)}, \\textbf{y}^{(m)})\\}_{m=1}^M$ and test set $\\{(\\textbf{u}^{(l)}, \\textbf{y}^{(l)})\\}_{l=1}^L$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb4e511-a8ba-4d70-b99b-5d02c8e90560",
   "metadata": {},
   "source": [
    "## Network architecture\n",
    "The Encoder network $\\Psi$ has:\n",
    "- 5 hidden layer with 500 units with ReLU activation function  \n",
    "- an input layer with $O$ number of nodes to match the dimension of our observational data $\\textbf{y}$\n",
    "- an output layer with dimension $2D$ to match the estimate posterior mean $\\mu_{post}$ and posterior covariance $\\Gamma_{post}$.  \n",
    "\n",
    "The decoder network $\\Psi_d$ has \n",
    "- 2 hidden layer with 500 nodes and relu activation function  \n",
    "- an input layer with $D$ number of nodes to represent a draw from the learned posterior\n",
    "- an output layer with $O$ nodes to match the dimension of the observational data.\n",
    "  \n",
    "For both encoder and decoder no activation function is used in the output layer.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ea68a5-f8e0-404c-a9c5-f560599f16c7",
   "metadata": {},
   "source": [
    "## Training \n",
    "For the optimization:\n",
    "- batch size = 100\n",
    "- Adam optimizer\n",
    "- 400 epochs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6bc6db-17e1-41b3-b8fa-005d28c47e9c",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "The metric used to measure the accuracy on the test set is the averaged relative error\n",
    "$$\n",
    " \\frac{1}{L} \\sum_{l=1}^L \\frac{\\lVert \\mathbf{y}^{(l)} - \\Psi_d(\\boldsymbol{\\mu}_{\\text{post}}(\\mathbf{y}^{(l)})) \\rVert_2^2}{\\lVert  \\mathbf{y}^{(l)}\\rVert_2^2}\n",
    "$$\n",
    "where $\\boldsymbol{\\mu}_{\\text{post}}(\\mathbf{y}^{(l)})$ is the estimated posterior mean fron the encoder network $\\Psi$ taking a datapoint $\\mathbf{y}^{(l)}$ as input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26351e39-abcc-474e-a3f6-85b4bef8cf79",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3087854-ee50-4332-beb3-eb5e960b4854",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5cf1ec6-bb10-420d-b5a2-1f29f74da36a",
   "metadata": {},
   "source": [
    "# References\n",
    "1. Solving Bayesian Inverse Problems via Variational Autoencoders, Hwan Goh, Sheroze Sheriffdeen, Jonathan Wittmer, Tan Bui-Thanh, \n",
    "https://doi.org/10.48550/arXiv.1912.04212"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayesian-env",
   "language": "python",
   "name": "bayesian-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
